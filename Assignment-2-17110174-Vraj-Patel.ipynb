{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import codecs\n",
    "from collections import Counter,defaultdict\n",
    "import nltk.data\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import PunktSentenceTokenizer, word_tokenize\n",
    "from functools import partial\n",
    "import random\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking data and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = \"\"\n",
    "with codecs.open(\"speeches.txt\",'r','UTF-8') as file:\n",
    "    raw_data = file.read()\n",
    "raw_data = re.sub(\"[\\s]+\",\" \",raw_data)\n",
    "raw_data = raw_data.replace(\"SPEECH\",\"\")\n",
    "raw_data = raw_data.replace(\"$\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(text):\n",
    "    text = text.replace(\"'\",\"\")\n",
    "    rx = re.compile(r\"[^a-z.?!’]+\")\n",
    "    return rx.sub(\" \",text)\n",
    "sent_tkn = PunktSentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sent_tkn.tokenize(raw_data)\n",
    "sentences = [remove_punc(i.lower().strip()) for i in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and Train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sents = len(sentences)\n",
    "random.seed(3000)\n",
    "random.shuffle(sentences) # shuffle the order of sentences\n",
    "train_num = 4*total_sents//5\n",
    "train_sents = sentences[:train_num]\n",
    "test_sents = sentences[train_num:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Gram Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Data Structures to ease N-Gram MLE Estimation and Perplexity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class partial_dd(defaultdict):\n",
    "    def __getitem__(self,key):\n",
    "        if(len(key) == len(list(self.keys())[0])):\n",
    "            return super().__getitem__(key)\n",
    "        else:\n",
    "            dd = defaultdict(self.default_factory)\n",
    "            for k,v in self.items():\n",
    "                if(k[:-1]==key):\n",
    "                    dd.update({k[-1]:v})\n",
    "            return dd\n",
    "def create_dd(dct,typ):\n",
    "    dd = partial_dd(typ)\n",
    "    dd.update(dct)\n",
    "    return dd\n",
    "float_dd = partial(create_dd,typ=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Functions for N-Gram Counts and MLE estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(sentences):\n",
    "    unigram = []\n",
    "    bigram = []\n",
    "    trigram = []\n",
    "    quadgram = []\n",
    "    for sentence in sentences:\n",
    "        tokens = ['<s>'] + word_tokenize(sentence) + ['</s>']\n",
    "        unigram.extend(list(ngrams(tokens,1)))\n",
    "        bigram.extend(list(ngrams(tokens,2)))\n",
    "        trigram.extend(list(ngrams(tokens,3)))\n",
    "        quadgram.extend(list(ngrams(tokens,4)))\n",
    "    total = len(unigram)\n",
    "    uni_count = Counter(unigram)\n",
    "    bi_count = Counter(bigram)\n",
    "    tri_count = Counter(trigram)\n",
    "    quad_count = Counter(quadgram)\n",
    "    return [total,uni_count,bi_count,tri_count,quad_count]\n",
    "\n",
    "def get_ngram_mles(ngram):\n",
    "    total = ngram[0] - ngram[1]['<s>']\n",
    "    uni_count = Counter(ngram[1])\n",
    "    bi_count = Counter(ngram[2])\n",
    "    tri_count = Counter(ngram[3])\n",
    "    quad_count = Counter(ngram[4])\n",
    "    uni_mle = float_dd({key: value/total for key,value in uni_count.items() if key!='<s>'})\n",
    "    bi_mle = float_dd({key: value/uni_count[key[:-1]] for key,value in bi_count.items()})\n",
    "    tri_mle = float_dd({key: value/bi_count[key[:-1]] for key,value in tri_count.items()})\n",
    "    quad_mle = float_dd({key: value/tri_count[key[:-1]] for key,value in quad_count.items()})\n",
    "    return [total,uni_mle,bi_mle,tri_mle,quad_mle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating MLE for Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ngrams = get_ngrams(train_sents)\n",
    "train_mles = get_ngram_mles(train_ngrams)\n",
    "vocab = set([i[0] for i in train_ngrams[1]])\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of the N-Grams Possible Vs. Actually Present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams Present: 5271\n",
      "Unigrams Possible: 5271\n",
      "Bigrams Present: 41674\n",
      "Bigrams Possible: 27783441\n",
      "Trigrams Present: 83459\n",
      "Trigrams Possible: 146446517511\n",
      "Quadgrams Present: 104851\n",
      "Quadgrams Possible: 771919593800481\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigrams Present:\",len(train_ngrams[1]))\n",
    "print(\"Unigrams Possible:\",vocab_size)\n",
    "print(\"Bigrams Present:\",len(train_ngrams[2]))\n",
    "print(\"Bigrams Possible:\",vocab_size**2)\n",
    "print(\"Trigrams Present:\",len(train_ngrams[3]))\n",
    "print(\"Trigrams Possible:\",vocab_size**3)\n",
    "print(\"Quadgrams Present:\",len(train_ngrams[4]))\n",
    "print(\"Quadgrams Possible:\",vocab_size**4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Funtions to Generate Random Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(starts,mles):\n",
    "    cond = tuple(starts)\n",
    "    gram = len(cond)+1\n",
    "#     print(gram,cond)\n",
    "    candidates = mles[gram][cond]\n",
    "    cand_repr = [i for i in candidates if i!='<s>']\n",
    "    cand_prob = [candidates[i] for i in cand_repr]\n",
    "    norm_probs = np.array(cand_prob)/sum(cand_prob)\n",
    "    experiments = np.random.multinomial(3,norm_probs)\n",
    "    return cand_repr[list(experiments).index(max(experiments))]\n",
    "def generate_rand(model_gram,mles):\n",
    "    rand_sent = ['<s>']\n",
    "    prevs = lambda x: [] if x==1 else rand_sent[-(model_gram-1):]\n",
    "    next_word = get_next(prevs(model_gram),mles)\n",
    "    while(next_word!='</s>'):\n",
    "        rand_sent.append(next_word)\n",
    "        next_word = get_next(prevs(model_gram),mles)\n",
    "    rand_sent.append(next_word)\n",
    "    return ' '.join(rand_sent)\n",
    "def pprint(sent):\n",
    "    s = sent.replace('<s> ','').replace('</s>','').replace(' .','.').replace(' !','!').replace(' ?','?').replace(\" ’ \",\"’\")\n",
    "    print(s,end = '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unigram Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "many and i i \n",
      "\n",
      "of you about. so re and believe of \n",
      "\n",
      "a me about because and.. \n",
      "\n",
      "you dollars i. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(5003)\n",
    "for i in range(5):\n",
    "    pprint(generate_rand(1,train_mles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigram Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you have to know it was on. \n",
      "\n",
      "i’t talk about it’re going to make our jobs. \n",
      "\n",
      "i think that’re doing a lot more. \n",
      "\n",
      "i think i want to be a lot of hispanics. \n",
      "\n",
      "and the poll trump? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1000)\n",
    "for i in range(5):\n",
    "    pprint(generate_rand(2,train_mles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigram Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have a very nice people. \n",
      "\n",
      "but it’s why being a real border. \n",
      "\n",
      "but i think that’s been a little bit of an executive order that president obama’s not going to take away americans’guns then admit the very rich guy very very successful real estate broker. \n",
      "\n",
      "it’s going to get rid of common core. \n",
      "\n",
      "and we’re going to do terrific and i’m talking about. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2000)\n",
    "for i in range(5):\n",
    "    pprint(generate_rand(3,train_mles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadgram Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i’m going to be a real wall. \n",
      "\n",
      "but i have to do it. \n",
      "\n",
      "you know we have a lot more. \n",
      "\n",
      "and i said i think i’d do and i feel terrible about the migration caused by hillary clinton and with all of her many problems and the tremendous mistakes that she’s being totally protected. \n",
      "\n",
      "you know i know you’re making less money. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3000)\n",
    "for i in range(5):\n",
    "    pprint(generate_rand(4,train_mles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments on readbility:\n",
    "- The **unigram model** fares the worst among all since it predicts even fullstops in the middle of the sentences according to my implementation (sentence end marker is not taken as full-stop as sentence might end with any of '.', '!' and '?'). No coherence in the words is present. Some sentences even end in 0 words.\n",
    "- The **bigram model** is better than unigram but it doesn't maintain grammar correctness for more than 2-3 words. Also there is the issue of the apostrophe appearing in wierd places. \n",
    "- The **trigram and quadgram models** get increasingly better and produce very readable and almost grammatically correct sentences. But as the sentence gets longer the context completely changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ngrams = get_ngrams(test_sents)\n",
    "vocab_test = set([i for i in test_ngrams[1]])\n",
    "vocab_tot_test = vocab | vocab_test\n",
    "vocab_tot_test_len = len(vocab_tot_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funtions for Perplexity Calculation\n",
    "Here I have used backoff with add-1 smoothing done for the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getprob(model,mles,seq):\n",
    "    if(seq==('<s>',)):\n",
    "        return 1\n",
    "    if(model==1):\n",
    "        if (seq[-1],) in mles[1]:\n",
    "#             print(\"waha\")\n",
    "            return (mles[1][(seq[-1],)]*(mles[0])+1)/(mles[0] + vocab_tot_test_len)\n",
    "        else:\n",
    "#             print(\"yaha\")\n",
    "            return 1/(mles[0] + vocab_tot_test_len)\n",
    "    elif seq[-model:] not in mles[model]:\n",
    "        return getprob(model-1,mles,seq)\n",
    "    else:\n",
    "#         print(\"yahi hai\")\n",
    "        return mles[model][seq[-model:]]\n",
    "def get_perplexity_help(sent,model,mles):\n",
    "    toks =['<s>'] + word_tokenize(sent) + ['<\\s>']\n",
    "    ppl = 0\n",
    "    for i in range(1,len(toks)+1):\n",
    "#         print(tuple(toks[max(0,i-model):i]),\":\",getprob(model,mles,tuple(toks[max(0,i-model):i])))\n",
    "        ppl -= np.log(getprob(model,mles,tuple(toks[max(0,i-model):i])))\n",
    "    return ppl\n",
    "\n",
    "def test_perplexity(sents,model,mles):\n",
    "    tot_p = 0\n",
    "    l_p = 0\n",
    "    for s in sents:\n",
    "        tot_p+=get_perplexity_help(s,model,mles)\n",
    "        l_p += (len(word_tokenize(s))+1)\n",
    "    return np.exp(tot_p/l_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity of N-Gram Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of unigram model = 608.6038028847778\n",
      "Perplexity of bigram model = 102.38938649249474\n",
      "Perplexity of trigram model = 66.05701130552193\n",
      "Perplexity of quadgram model = 59.4118704289358\n"
     ]
    }
   ],
   "source": [
    "ppl_unigram = test_perplexity(test_sents,1,train_mles)\n",
    "ppl_bigram = test_perplexity(test_sents,2,train_mles)\n",
    "ppl_trigram = test_perplexity(test_sents,3,train_mles)\n",
    "ppl_quadgram = test_perplexity(test_sents,4,train_mles)\n",
    "print(\"Perplexity of unigram model =\",ppl_unigram)\n",
    "print(\"Perplexity of bigram model =\",ppl_bigram)\n",
    "print(\"Perplexity of trigram model =\",ppl_trigram)\n",
    "print(\"Perplexity of quadgram model =\",ppl_quadgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Vocabulary and Useful Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_vocab =set([i for sen in sentences for i in word_tokenize(sen)])\n",
    "tot_vocab = list(tot_vocab)\n",
    "tot_vocab = ['<pad>','<s>','</s>'] + tot_vocab\n",
    "tot_vocab_len = len(tot_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs = keras.utils.to_categorical([i for i in range(tot_vocab_len)],num_classes=tot_vocab_len)\n",
    "vec_map = {k:v for k,v in zip(tot_vocab,vecs)}\n",
    "word2int_map = {k:v for k,v in zip(tot_vocab,[i for i in range(tot_vocab_len)])}\n",
    "int2word_map = {v:k for k,v in zip(tot_vocab,[i for i in range(tot_vocab_len)])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Functions for Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 20\n",
    "def prep_sent(sent):\n",
    "    toks = word_tokenize(sent)\n",
    "#     print(toks)\n",
    "    return [word2int_map[i] for i in toks]\n",
    "\n",
    "def trim_and_pad(x_inp,length):\n",
    "    if(len(x_inp)>length):\n",
    "        return np.array(x_inp[-length:])\n",
    "    else:\n",
    "        a = np.array(x_inp)\n",
    "        a.resize((length,))\n",
    "        return a\n",
    "\n",
    "def preprocess_data(sent_list):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    for sent in sent_list:\n",
    "        sent_encode = [word2int_map['<s>']]+ prep_sent(sent)\n",
    "        x_data.extend([sent_encode[:i] for i in range(1,len(sent_encode)+1)])\n",
    "        y_data.extend([word2int_map[i] for i in word_tokenize(sent)]+[word2int_map['</s>']])\n",
    "    x_data = [trim_and_pad(x,max_len) for x in x_data]\n",
    "    y_data = keras.utils.to_categorical(y_data,num_classes=tot_vocab_len)\n",
    "    return np.array(x_data),y_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors, labels = preprocess_data(train_sents)\n",
    "test_input, test_label = preprocess_data(test_sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the RNN Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"rnn_language_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence (InputLayer)        [(None, 20)]              0         \n",
      "_________________________________________________________________\n",
      "word_embedding (Embedding)   (None, 20, 128)           738432    \n",
      "_________________________________________________________________\n",
      "RNN_layer (SimpleRNN)        (None, 256)               98560     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "predicted_word (Dense)       (None, 5769)              1482633   \n",
      "=================================================================\n",
      "Total params: 2,319,625\n",
      "Trainable params: 2,319,625\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_inp = 20\n",
    "input_s = keras.Input(shape=(max_inp,),name='sentence')\n",
    "word_features = keras.layers.Embedding(tot_vocab_len,128,input_length=max_inp,name='word_embedding')(input_s)\n",
    "rnn_layer = keras.layers.SimpleRNN(256,name='RNN_layer')(word_features)\n",
    "rnn_layer = keras.layers.Dropout(0.2)(rnn_layer)\n",
    "output_layer = keras.layers.Dense(tot_vocab_len,activation = 'softmax', name='predicted_word')(rnn_layer)\n",
    "model_rnn = keras.Model(inputs= input_s,outputs = output_layer, name='rnn_language_model')\n",
    "checkpoint_gen = keras.callbacks.ModelCheckpoint(filepath='rnn_weights_3.hdf5', verbose=1)\n",
    "model_rnn.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model_rnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 171875 samples\n",
      "Epoch 1/20\n",
      "171840/171875 [============================>.] - ETA: 0s - loss: 5.3567 - accuracy: 0.1546\n",
      "Epoch 00001: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 59s 342us/sample - loss: 5.3566 - accuracy: 0.1546\n",
      "Epoch 2/20\n",
      "171712/171875 [============================>.] - ETA: 0s - loss: 4.7653 - accuracy: 0.2120\n",
      "Epoch 00002: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 58s 337us/sample - loss: 4.7653 - accuracy: 0.2121\n",
      "Epoch 3/20\n",
      "171840/171875 [============================>.] - ETA: 0s - loss: 4.5452 - accuracy: 0.2413\n",
      "Epoch 00003: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 56s 328us/sample - loss: 4.5452 - accuracy: 0.2413\n",
      "Epoch 4/20\n",
      "171712/171875 [============================>.] - ETA: 0s - loss: 4.3752 - accuracy: 0.2617\n",
      "Epoch 00004: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 58s 336us/sample - loss: 4.3754 - accuracy: 0.2617\n",
      "Epoch 5/20\n",
      "171840/171875 [============================>.] - ETA: 0s - loss: 4.2613 - accuracy: 0.2723\n",
      "Epoch 00005: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 56s 327us/sample - loss: 4.2613 - accuracy: 0.2723\n",
      "Epoch 6/20\n",
      "171840/171875 [============================>.] - ETA: 0s - loss: 4.1773 - accuracy: 0.2775\n",
      "Epoch 00006: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 56s 324us/sample - loss: 4.1774 - accuracy: 0.2775\n",
      "Epoch 7/20\n",
      "171840/171875 [============================>.] - ETA: 0s - loss: 4.1034 - accuracy: 0.2836\n",
      "Epoch 00007: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 56s 327us/sample - loss: 4.1035 - accuracy: 0.2836\n",
      "Epoch 8/20\n",
      "171776/171875 [============================>.] - ETA: 0s - loss: 4.0384 - accuracy: 0.2900\n",
      "Epoch 00008: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 56s 323us/sample - loss: 4.0383 - accuracy: 0.2900\n",
      "Epoch 9/20\n",
      "171776/171875 [============================>.] - ETA: 0s - loss: 3.9911 - accuracy: 0.2922\n",
      "Epoch 00009: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 55s 320us/sample - loss: 3.9910 - accuracy: 0.2922\n",
      "Epoch 10/20\n",
      "171840/171875 [============================>.] - ETA: 0s - loss: 3.9522 - accuracy: 0.2953\n",
      "Epoch 00010: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 55s 322us/sample - loss: 3.9520 - accuracy: 0.2953\n",
      "Epoch 11/20\n",
      "171776/171875 [============================>.] - ETA: 0s - loss: 3.9246 - accuracy: 0.2977\n",
      "Epoch 00011: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 55s 318us/sample - loss: 3.9246 - accuracy: 0.2977\n",
      "Epoch 12/20\n",
      "171840/171875 [============================>.] - ETA: 0s - loss: 3.8812 - accuracy: 0.3011\n",
      "Epoch 00012: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 55s 319us/sample - loss: 3.8813 - accuracy: 0.3011\n",
      "Epoch 13/20\n",
      "171712/171875 [============================>.] - ETA: 0s - loss: 3.8533 - accuracy: 0.3035\n",
      "Epoch 00013: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 57s 330us/sample - loss: 3.8537 - accuracy: 0.3035\n",
      "Epoch 14/20\n",
      "171712/171875 [============================>.] - ETA: 0s - loss: 3.8304 - accuracy: 0.3061\n",
      "Epoch 00014: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 58s 336us/sample - loss: 3.8303 - accuracy: 0.3062\n",
      "Epoch 15/20\n",
      "171712/171875 [============================>.] - ETA: 0s - loss: 3.8253 - accuracy: 0.3063\n",
      "Epoch 00015: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 56s 327us/sample - loss: 3.8253 - accuracy: 0.3063\n",
      "Epoch 16/20\n",
      "171776/171875 [============================>.] - ETA: 0s - loss: 3.8030 - accuracy: 0.3096\n",
      "Epoch 00016: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 57s 331us/sample - loss: 3.8031 - accuracy: 0.3096\n",
      "Epoch 17/20\n",
      "171776/171875 [============================>.] - ETA: 0s - loss: 3.7794 - accuracy: 0.3104\n",
      "Epoch 00017: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 56s 326us/sample - loss: 3.7795 - accuracy: 0.3104\n",
      "Epoch 18/20\n",
      "171840/171875 [============================>.] - ETA: 0s - loss: 3.7754 - accuracy: 0.3111\n",
      "Epoch 00018: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 57s 331us/sample - loss: 3.7755 - accuracy: 0.3111\n",
      "Epoch 19/20\n",
      "171776/171875 [============================>.] - ETA: 0s - loss: 3.7659 - accuracy: 0.3120\n",
      "Epoch 00019: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 57s 331us/sample - loss: 3.7662 - accuracy: 0.3120\n",
      "Epoch 20/20\n",
      "171712/171875 [============================>.] - ETA: 0s - loss: 3.7539 - accuracy: 0.3137\n",
      "Epoch 00020: saving model to rnn_weights_3.hdf5\n",
      "171875/171875 [==============================] - 56s 326us/sample - loss: 3.7542 - accuracy: 0.3137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e9acadf208>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.fit(predictors,labels,batch_size=64 ,epochs=20,verbose=1,callbacks=[checkpoint_gen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the LSTM Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"lstm_language_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_lstm (InputLayer)   [(None, 20)]              0         \n",
      "_________________________________________________________________\n",
      "word_embedding_lstm (Embeddi (None, 20, 128)           738432    \n",
      "_________________________________________________________________\n",
      "LSTM_layer (LSTM)            (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "predicted_word (Dense)       (None, 5769)              1482633   \n",
      "=================================================================\n",
      "Total params: 2,615,305\n",
      "Trainable params: 2,615,305\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "max_inp = 20\n",
    "input_s_lstm = keras.Input(shape=(max_inp,),name='sentence_lstm')\n",
    "word_features_lstm = keras.layers.Embedding(tot_vocab_len,128,input_length=max_inp,name='word_embedding_lstm')(input_s_lstm)\n",
    "lstm_layer = keras.layers.LSTM(256,name='LSTM_layer',return_sequences=False)(word_features_lstm)\n",
    "lstm_layer = keras.layers.Dropout(0.2)(lstm_layer)\n",
    "output_layer_lstm = keras.layers.Dense(tot_vocab_len,activation = 'softmax', name='predicted_word')(lstm_layer)\n",
    "model_lstm = keras.Model(inputs= input_s_lstm,outputs = output_layer_lstm, name='lstm_language_model')\n",
    "checkpoint_gen = keras.callbacks.ModelCheckpoint(filepath='lstm_weights_3.hdf5', verbose=1)\n",
    "model_lstm.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 171875 samples\n",
      "Epoch 1/20\n",
      "171712/171875 [============================>.] - ETA: 0s - loss: 5.6451 - accuracy: 0.0810\n",
      "Epoch 00001: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 48s 276us/sample - loss: 5.6450 - accuracy: 0.0810\n",
      "Epoch 2/20\n",
      "171776/171875 [============================>.] - ETA: 0s - loss: 5.3014 - accuracy: 0.1502\n",
      "Epoch 00002: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 45s 264us/sample - loss: 5.3013 - accuracy: 0.1502\n",
      "Epoch 3/20\n",
      "171712/171875 [============================>.] - ETA: 0s - loss: 4.7516 - accuracy: 0.2141\n",
      "Epoch 00003: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 45s 262us/sample - loss: 4.7513 - accuracy: 0.2141\n",
      "Epoch 4/20\n",
      "171840/171875 [============================>.] - ETA: 0s - loss: 4.3067 - accuracy: 0.2690\n",
      "Epoch 00004: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 45s 264us/sample - loss: 4.3067 - accuracy: 0.2691\n",
      "Epoch 5/20\n",
      "171712/171875 [============================>.] - ETA: 0s - loss: 4.0460 - accuracy: 0.2957\n",
      "Epoch 00005: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 45s 262us/sample - loss: 4.0458 - accuracy: 0.2958\n",
      "Epoch 6/20\n",
      "171776/171875 [============================>.] - ETA: 0s - loss: 3.8675 - accuracy: 0.3085\n",
      "Epoch 00006: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 46s 265us/sample - loss: 3.8673 - accuracy: 0.3085\n",
      "Epoch 7/20\n",
      "171648/171875 [============================>.] - ETA: 0s - loss: 3.7189 - accuracy: 0.3180\n",
      "Epoch 00007: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 45s 261us/sample - loss: 3.7189 - accuracy: 0.3180\n",
      "Epoch 8/20\n",
      "171840/171875 [============================>.] - ETA: 0s - loss: 3.5864 - accuracy: 0.3276\n",
      "Epoch 00008: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 45s 264us/sample - loss: 3.5863 - accuracy: 0.3276\n",
      "Epoch 9/20\n",
      "171776/171875 [============================>.] - ETA: 0s - loss: 3.4622 - accuracy: 0.3375\n",
      "Epoch 00009: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 45s 263us/sample - loss: 3.4620 - accuracy: 0.3375\n",
      "Epoch 10/20\n",
      "171776/171875 [============================>.] - ETA: 0s - loss: 3.3475 - accuracy: 0.3482\n",
      "Epoch 00010: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 46s 266us/sample - loss: 3.3475 - accuracy: 0.3482\n",
      "Epoch 11/20\n",
      "171648/171875 [============================>.] - ETA: 0s - loss: 3.2395 - accuracy: 0.3577\n",
      "Epoch 00011: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 45s 262us/sample - loss: 3.2396 - accuracy: 0.3576\n",
      "Epoch 12/20\n",
      "171712/171875 [============================>.] - ETA: 0s - loss: 3.1411 - accuracy: 0.3682\n",
      "Epoch 00012: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 46s 266us/sample - loss: 3.1408 - accuracy: 0.3682\n",
      "Epoch 13/20\n",
      "171840/171875 [============================>.] - ETA: 0s - loss: 3.0452 - accuracy: 0.3785\n",
      "Epoch 00013: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 45s 261us/sample - loss: 3.0452 - accuracy: 0.3785\n",
      "Epoch 14/20\n",
      "171840/171875 [============================>.] - ETA: 0s - loss: 2.9560 - accuracy: 0.3878\n",
      "Epoch 00014: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 45s 261us/sample - loss: 2.9559 - accuracy: 0.3878\n",
      "Epoch 15/20\n",
      "171776/171875 [============================>.] - ETA: 0s - loss: 2.8712 - accuracy: 0.3972\n",
      "Epoch 00015: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 45s 263us/sample - loss: 2.8712 - accuracy: 0.3972\n",
      "Epoch 16/20\n",
      "171712/171875 [============================>.] - ETA: 0s - loss: 2.7897 - accuracy: 0.4074\n",
      "Epoch 00016: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 45s 260us/sample - loss: 2.7899 - accuracy: 0.4073\n",
      "Epoch 17/20\n",
      "171648/171875 [============================>.] - ETA: 0s - loss: 2.7138 - accuracy: 0.4184\n",
      "Epoch 00017: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 45s 261us/sample - loss: 2.7142 - accuracy: 0.4184\n",
      "Epoch 18/20\n",
      "171776/171875 [============================>.] - ETA: 0s - loss: 2.6436 - accuracy: 0.4263\n",
      "Epoch 00018: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 45s 263us/sample - loss: 2.6437 - accuracy: 0.4263\n",
      "Epoch 19/20\n",
      "171776/171875 [============================>.] - ETA: 0s - loss: 2.5791 - accuracy: 0.4372\n",
      "Epoch 00019: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 45s 262us/sample - loss: 2.5792 - accuracy: 0.4371\n",
      "Epoch 20/20\n",
      "171648/171875 [============================>.] - ETA: 0s - loss: 2.5152 - accuracy: 0.4471\n",
      "Epoch 00020: saving model to lstm_weights_3.hdf5\n",
      "171875/171875 [==============================] - 45s 262us/sample - loss: 2.5151 - accuracy: 0.4472\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ea90db43c8>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.fit(predictors,labels,batch_size=64 ,epochs=20,verbose=1,callbacks=[checkpoint_gen])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Functions to get Perplexity and Random Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neural_perplexity(model,sents,labels):\n",
    "    predictions = model.predict(sents)\n",
    "    ppl = 0\n",
    "    for i in range(len(predictions)):\n",
    "        ppl -= np.log(predictions[i][labels[i].argmax(axis=-1)])\n",
    "    return np.exp(ppl/(len(predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_random(prob_vec):\n",
    "#     nrm = np.linalg.norm(prob_vec,2)\n",
    "    nrm  = 1.001*np.sum(prob_vec)\n",
    "    nrm_prob = prob_vec/nrm\n",
    "#     print(np.sum(nrm_prob))\n",
    "    ans = np.random.multinomial(3,nrm_prob)\n",
    "    return ans.argmax(axis=-1)\n",
    "\n",
    "def get_random_sents(model,max_len,vec_len):\n",
    "    curr_inp = [word2int_map['<s>']]\n",
    "    curr_inp = trim_and_pad(curr_inp,vec_len)\n",
    "    sent = []\n",
    "    sent.append(word2int_map['<s>'])\n",
    "    curr_vec = model.predict(np.array([curr_inp]))[0]\n",
    "    curr_op = get_next_random(curr_vec)\n",
    "    iters = 1\n",
    "    while (curr_op!=word2int_map['</s>'] and iters!=max_len):\n",
    "        sent.append(curr_op)\n",
    "        curr_inp = trim_and_pad(sent[-vec_len:],vec_len)\n",
    "        curr_vec = model.predict(np.array([curr_inp]))[0]\n",
    "#         print(*curr_vec)\n",
    "        curr_op = get_next_random(curr_vec)\n",
    "        iters+=1\n",
    "    return ' '.join([int2word_map[i] for i in sent][1:]).replace(' .','.').replace(' !','!').replace(' ?','?').replace(\" ’ \",\"’\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of vanilla RNN model= 87.984368499613\n"
     ]
    }
   ],
   "source": [
    "rnn_pplx = get_neural_perplexity(model_rnn,test_input[:],test_label[:])\n",
    "print(\"Perplexity of vanilla RNN model=\",rnn_pplx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but what was soon no nice television just cuts.\n",
      "\n",
      "i’ll be a scam more trait.\n",
      "\n",
      "and i mean no a memo.\n",
      "\n",
      "and more.\n",
      "\n",
      "iran thank i’m they build repealing i again is a summit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3001)\n",
    "for i in range(5):\n",
    "    print(get_random_sents(model_rnn,30,20),end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity of LSTM model= 63.81389231418633\n"
     ]
    }
   ],
   "source": [
    "lstm_pplx = get_neural_perplexity(model_lstm,test_input[:],test_label[:])\n",
    "print(\"Perplexity of LSTM model=\",lstm_pplx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but what i did hard is she was going to come in and it was so unfair really.\n",
      "\n",
      "she can’t even hear the name of the bad party.\n",
      "\n",
      "i was saying why ivanka is i won a couple of months ago.\n",
      "\n",
      "san pacs are buying in many cases and keep education and saying and i see what this many is ridiculous.\n",
      "\n",
      "and honestly i’m a rough place.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(3001)\n",
    "for i in range(5):\n",
    "    print(get_random_sents(model_lstm,30,20),end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comment on readability:\n",
    "Both RNN and LSTM produce results which are quite readable but LSTM fares much better in terms of readability as there are less grammatical mistakes and context is maintained for a longer time and sentences are less awkward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Classical and Neural Approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Classical Approaches:\n",
      "------------------------------------------------------------\n",
      "Perplexity of unigram model = 608.6038028847778\n",
      "Perplexity of bigram model = 102.38938649249474\n",
      "Perplexity of trigram model = 66.05701130552193\n",
      "Perplexity of quadgram model = 59.4118704289358\n",
      "\n",
      "------------------------------------------------------------\n",
      "Neural Approaches:\n",
      "------------------------------------------------------------\n",
      "Perplexity of vanilla RNN model= 87.984368499613\n",
      "Perplexity of LSTM model= 63.81389231418633\n"
     ]
    }
   ],
   "source": [
    "print(\"-\"*60)\n",
    "print(\"Classical Approaches:\")\n",
    "print(\"-\"*60)\n",
    "print(\"Perplexity of unigram model =\",ppl_unigram)\n",
    "print(\"Perplexity of bigram model =\",ppl_bigram)\n",
    "print(\"Perplexity of trigram model =\",ppl_trigram)\n",
    "print(\"Perplexity of quadgram model =\",ppl_quadgram)\n",
    "print('')\n",
    "print(\"-\"*60)\n",
    "print(\"Neural Approaches:\")\n",
    "print(\"-\"*60)\n",
    "print(\"Perplexity of vanilla RNN model=\",rnn_pplx)\n",
    "print(\"Perplexity of LSTM model=\",lstm_pplx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we see that the **RNN model** perform better than **bigram** but worse than **trigram** and **quadgram**.\n",
    "- We also see that the **LSTM model** perform better better than even **trigram** but falls little short of the performace of **quadgram model**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The classical approach seems to work better on this dataset. \n",
    "- It might be due to the limited vocabulary and the shorter and simpler sentence construction in Donald Trump's speeches. \n",
    "- But Neural models, especially LSTM, might generalize better for this data if trained more on this dataset as they do have the potential to imitate the N-Gram models if they have to. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
